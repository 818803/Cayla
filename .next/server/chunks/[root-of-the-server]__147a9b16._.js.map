{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 6, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"","debugId":null}},
    {"offset": {"line": 60, "column": 0}, "map": {"version":3,"sources":["file:///Users/plana/Desktop/test/src/lib/llm-service.ts"],"sourcesContent":["// lib/llm-service.ts\nexport interface LLMMessage {\n  role: 'user' | 'assistant' | 'system';\n  content: string;\n}\n\nexport interface LLMResponse {\n  message: string;\n  error?: string;\n  provider?: string;\n  tokenUsage?: {\n    prompt: number;\n    completion: number;\n    total: number;\n  };\n}\n\nexport interface LLMConfig {\n  apiKey: string;\n  model?: string;\n  maxTokens?: number;\n  temperature?: number;\n  topP?: number;\n  frequencyPenalty?: number;\n  presencePenalty?: number;\n}\n\nexport type LLMProvider = 'openai' | 'anthropic' | 'auto';\n\nexport class LLMService {\n  private config: LLMConfig;\n  private provider: LLMProvider;\n  private fallbackResponses: string[] = [\n    \"Dunno\",\n  ];\n\n  constructor(config: LLMConfig, provider: LLMProvider = 'auto') {\n    this.config = {\n      maxTokens: 1500,\n      temperature: 0.7,\n      topP: 1,\n      frequencyPenalty: 0,\n      presencePenalty: 0,\n      ...config\n    };\n    this.provider = provider;\n  }\n\n  async generateResponse(messages: LLMMessage[]): Promise<LLMResponse> {\n    // Validate input\n    if (!messages || messages.length === 0) {\n      return {\n        message: \"I didn't receive any message to respond to. What would you like to talk about?\",\n        error: \"No messages provided\"\n      };\n    }\n\n    // Auto-detect provider or use specified one\n    const selectedProvider = this.provider === 'auto' ? this.detectBestProvider() : this.provider;\n\n    try {\n      switch (selectedProvider) {\n        case 'anthropic':\n          return await this.generateResponseClaude(messages);\n        case 'openai':\n        default:\n          return await this.generateResponseOpenAI(messages);\n      }\n    } catch (error) {\n      return this.handleError(error);\n    }\n  }\n\n  private async generateResponseOpenAI(messages: LLMMessage[]): Promise<LLMResponse> {\n    const response = await fetch('https://api.openai.com/v1/chat/completions', {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json',\n        'Authorization': `Bearer ${this.config.apiKey}`,\n      },\n      body: JSON.stringify({\n        model: this.config.model || 'gpt-3.5-turbo',\n        messages: messages,\n        max_tokens: this.config.maxTokens,\n        temperature: this.config.temperature,\n        top_p: this.config.topP,\n        frequency_penalty: this.config.frequencyPenalty,\n        presence_penalty: this.config.presencePenalty,\n      }),\n    });\n\n    if (!response.ok) {\n      throw new Error(`OpenAI API error: ${response.status} - ${await response.text()}`);\n    }\n\n    const data = await response.json();\n    \n    if (!data.choices || data.choices.length === 0) {\n      throw new Error('No response generated from OpenAI');\n    }\n\n    return {\n      message: data.choices[0].message.content,\n      provider: 'openai',\n      tokenUsage: data.usage ? {\n        prompt: data.usage.prompt_tokens,\n        completion: data.usage.completion_tokens,\n        total: data.usage.total_tokens\n      } : undefined\n    };\n  }\n\n  private async generateResponseClaude(messages: LLMMessage[]): Promise<LLMResponse> {\n    const systemMessage = messages.find(m => m.role === 'system');\n    const conversationMessages = messages.filter(m => m.role !== 'system');\n\n    const response = await fetch('https://api.anthropic.com/v1/messages', {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json',\n        'Authorization': `Bearer ${this.config.apiKey}`,\n        'anthropic-version': '2023-06-01',\n      },\n      body: JSON.stringify({\n        model: this.config.model || 'claude-3-sonnet-20240229',\n        max_tokens: this.config.maxTokens,\n        messages: conversationMessages,\n        system: systemMessage?.content,\n        temperature: this.config.temperature,\n        top_p: this.config.topP,\n      }),\n    });\n\n    if (!response.ok) {\n      throw new Error(`Claude API error: ${response.status} - ${await response.text()}`);\n    }\n\n    const data = await response.json();\n    \n    if (!data.content || data.content.length === 0) {\n      throw new Error('No response generated from Claude');\n    }\n\n    return {\n      message: data.content[0].text,\n      provider: 'anthropic',\n      tokenUsage: data.usage ? {\n        prompt: data.usage.input_tokens,\n        completion: data.usage.output_tokens,\n        total: data.usage.input_tokens + data.usage.output_tokens\n      } : undefined\n    };\n  }\n\n  private detectBestProvider(): LLMProvider {\n    // Simple logic to determine best provider\n    // You can enhance this based on your needs\n    if (this.config.model?.includes('claude')) {\n      return 'anthropic';\n    }\n    return 'openai';\n  }\n\n  private handleError(error: unknown): LLMResponse {\n    console.error('LLM Service Error:', error);\n    \n    let contextualMessage = this.getRandomFallbackResponse();\n    let errorMessage = 'Unknown error';\n\n    if (error instanceof Error) {\n      errorMessage = error.message;\n      \n      // Provide contextual responses based on error type\n      if (error.message.includes('401') || error.message.includes('unauthorized')) {\n        contextualMessage = \"It looks like there's an authentication issue. Please check your API key configuration.\";\n      } else if (error.message.includes('429') || error.message.includes('rate limit')) {\n        contextualMessage = \"I'm getting too many requests right now. Please wait a moment and try again.\";\n      } else if (error.message.includes('400') || error.message.includes('bad request')) {\n        contextualMessage = \"There seems to be an issue with how your request was formatted. Could you try rephrasing?\";\n      } else if (error.message.includes('500') || error.message.includes('internal server')) {\n        contextualMessage = \"The AI service is experiencing technical difficulties. Please try again in a few minutes.\";\n      } else if (error.message.includes('network') || error.message.includes('fetch')) {\n        contextualMessage = \"I'm having trouble connecting to the AI service. Please check your internet connection and try again.\";\n      }\n    }\n\n    return {\n      message: contextualMessage,\n      error: errorMessage\n    };\n  }\n\n  private getRandomFallbackResponse(): string {\n    return this.fallbackResponses[Math.floor(Math.random() * this.fallbackResponses.length)];\n  }\n\n  // Utility methods\n  async streamResponse(messages: LLMMessage[], onChunk: (chunk: string) => void): Promise<LLMResponse> {\n    // Implementation for streaming responses\n    try {\n      const response = await fetch('https://api.openai.com/v1/chat/completions', {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json',\n          'Authorization': `Bearer ${this.config.apiKey}`,\n        },\n        body: JSON.stringify({\n          model: this.config.model || 'gpt-3.5-turbo',\n          messages: messages,\n          max_tokens: this.config.maxTokens,\n          temperature: this.config.temperature,\n          stream: true,\n        }),\n      });\n\n      if (!response.ok) {\n        throw new Error(`Streaming API error: ${response.status}`);\n      }\n\n      const reader = response.body?.getReader();\n      if (!reader) {\n        throw new Error('No response stream available');\n      }\n\n      let fullMessage = '';\n      const decoder = new TextDecoder();\n\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n\n        const chunk = decoder.decode(value);\n        const lines = chunk.split('\\n').filter(line => line.trim());\n\n        for (const line of lines) {\n          if (line.startsWith('data: ')) {\n            const data = line.slice(6);\n            if (data === '[DONE]') continue;\n\n            try {\n              const parsed = JSON.parse(data);\n              const content = parsed.choices?.[0]?.delta?.content;\n              if (content) {\n                fullMessage += content;\n                onChunk(content);\n              }\n            } catch (e) {\n              // Skip malformed JSON\n              continue;\n            }\n          }\n        }\n      }\n\n      return {\n        message: fullMessage,\n        provider: 'openai'\n      };\n    } catch (error) {\n      return this.handleError(error);\n    }\n  }\n\n  // Method to check API health\n  async healthCheck(): Promise<{ status: 'healthy' | 'unhealthy'; provider: string }> {\n    try {\n      const testMessage: LLMMessage[] = [\n        { role: 'user', content: 'Hello' }\n      ];\n      \n      const response = await this.generateResponse(testMessage);\n      \n      return {\n        status: response.error ? 'unhealthy' : 'healthy',\n        provider: response.provider || 'unknown'\n      };\n    } catch (error) {\n      return {\n        status: 'unhealthy',\n        provider: 'unknown'\n      };\n    }\n  }\n\n  // Method to estimate token count (rough approximation)\n  estimateTokens(text: string): number {\n    // Rough estimation: ~4 characters per token for English text\n    return Math.ceil(text.length / 4);\n  }\n\n  // Method to truncate messages if they exceed token limits\n  truncateMessages(messages: LLMMessage[], maxTokens: number): LLMMessage[] {\n    let totalTokens = 0;\n    const truncatedMessages: LLMMessage[] = [];\n\n    // Always keep system message if present\n    const systemMessage = messages.find(m => m.role === 'system');\n    if (systemMessage) {\n      totalTokens += this.estimateTokens(systemMessage.content);\n      truncatedMessages.push(systemMessage);\n    }\n\n    // Add messages from most recent backwards\n    const conversationMessages = messages.filter(m => m.role !== 'system').reverse();\n    \n    for (const message of conversationMessages) {\n      const messageTokens = this.estimateTokens(message.content);\n      if (totalTokens + messageTokens <= maxTokens) {\n        totalTokens += messageTokens;\n        truncatedMessages.unshift(message);\n      } else {\n        break;\n      }\n    }\n\n    return truncatedMessages;\n  }\n}"],"names":[],"mappings":"AAAA,qBAAqB;;;;AA6Bd,MAAM;IACH,OAAkB;IAClB,SAAsB;IACtB,oBAA8B;QACpC;KACD,CAAC;IAEF,YAAY,MAAiB,EAAE,WAAwB,MAAM,CAAE;QAC7D,IAAI,CAAC,MAAM,GAAG;YACZ,WAAW;YACX,aAAa;YACb,MAAM;YACN,kBAAkB;YAClB,iBAAiB;YACjB,GAAG,MAAM;QACX;QACA,IAAI,CAAC,QAAQ,GAAG;IAClB;IAEA,MAAM,iBAAiB,QAAsB,EAAwB;QACnE,iBAAiB;QACjB,IAAI,CAAC,YAAY,SAAS,MAAM,KAAK,GAAG;YACtC,OAAO;gBACL,SAAS;gBACT,OAAO;YACT;QACF;QAEA,4CAA4C;QAC5C,MAAM,mBAAmB,IAAI,CAAC,QAAQ,KAAK,SAAS,IAAI,CAAC,kBAAkB,KAAK,IAAI,CAAC,QAAQ;QAE7F,IAAI;YACF,OAAQ;gBACN,KAAK;oBACH,OAAO,MAAM,IAAI,CAAC,sBAAsB,CAAC;gBAC3C,KAAK;gBACL;oBACE,OAAO,MAAM,IAAI,CAAC,sBAAsB,CAAC;YAC7C;QACF,EAAE,OAAO,OAAO;YACd,OAAO,IAAI,CAAC,WAAW,CAAC;QAC1B;IACF;IAEA,MAAc,uBAAuB,QAAsB,EAAwB;QACjF,MAAM,WAAW,MAAM,MAAM,8CAA8C;YACzE,QAAQ;YACR,SAAS;gBACP,gBAAgB;gBAChB,iBAAiB,CAAC,OAAO,EAAE,IAAI,CAAC,MAAM,CAAC,MAAM,EAAE;YACjD;YACA,MAAM,KAAK,SAAS,CAAC;gBACnB,OAAO,IAAI,CAAC,MAAM,CAAC,KAAK,IAAI;gBAC5B,UAAU;gBACV,YAAY,IAAI,CAAC,MAAM,CAAC,SAAS;gBACjC,aAAa,IAAI,CAAC,MAAM,CAAC,WAAW;gBACpC,OAAO,IAAI,CAAC,MAAM,CAAC,IAAI;gBACvB,mBAAmB,IAAI,CAAC,MAAM,CAAC,gBAAgB;gBAC/C,kBAAkB,IAAI,CAAC,MAAM,CAAC,eAAe;YAC/C;QACF;QAEA,IAAI,CAAC,SAAS,EAAE,EAAE;YAChB,MAAM,IAAI,MAAM,CAAC,kBAAkB,EAAE,SAAS,MAAM,CAAC,GAAG,EAAE,MAAM,SAAS,IAAI,IAAI;QACnF;QAEA,MAAM,OAAO,MAAM,SAAS,IAAI;QAEhC,IAAI,CAAC,KAAK,OAAO,IAAI,KAAK,OAAO,CAAC,MAAM,KAAK,GAAG;YAC9C,MAAM,IAAI,MAAM;QAClB;QAEA,OAAO;YACL,SAAS,KAAK,OAAO,CAAC,EAAE,CAAC,OAAO,CAAC,OAAO;YACxC,UAAU;YACV,YAAY,KAAK,KAAK,GAAG;gBACvB,QAAQ,KAAK,KAAK,CAAC,aAAa;gBAChC,YAAY,KAAK,KAAK,CAAC,iBAAiB;gBACxC,OAAO,KAAK,KAAK,CAAC,YAAY;YAChC,IAAI;QACN;IACF;IAEA,MAAc,uBAAuB,QAAsB,EAAwB;QACjF,MAAM,gBAAgB,SAAS,IAAI,CAAC,CAAA,IAAK,EAAE,IAAI,KAAK;QACpD,MAAM,uBAAuB,SAAS,MAAM,CAAC,CAAA,IAAK,EAAE,IAAI,KAAK;QAE7D,MAAM,WAAW,MAAM,MAAM,yCAAyC;YACpE,QAAQ;YACR,SAAS;gBACP,gBAAgB;gBAChB,iBAAiB,CAAC,OAAO,EAAE,IAAI,CAAC,MAAM,CAAC,MAAM,EAAE;gBAC/C,qBAAqB;YACvB;YACA,MAAM,KAAK,SAAS,CAAC;gBACnB,OAAO,IAAI,CAAC,MAAM,CAAC,KAAK,IAAI;gBAC5B,YAAY,IAAI,CAAC,MAAM,CAAC,SAAS;gBACjC,UAAU;gBACV,QAAQ,eAAe;gBACvB,aAAa,IAAI,CAAC,MAAM,CAAC,WAAW;gBACpC,OAAO,IAAI,CAAC,MAAM,CAAC,IAAI;YACzB;QACF;QAEA,IAAI,CAAC,SAAS,EAAE,EAAE;YAChB,MAAM,IAAI,MAAM,CAAC,kBAAkB,EAAE,SAAS,MAAM,CAAC,GAAG,EAAE,MAAM,SAAS,IAAI,IAAI;QACnF;QAEA,MAAM,OAAO,MAAM,SAAS,IAAI;QAEhC,IAAI,CAAC,KAAK,OAAO,IAAI,KAAK,OAAO,CAAC,MAAM,KAAK,GAAG;YAC9C,MAAM,IAAI,MAAM;QAClB;QAEA,OAAO;YACL,SAAS,KAAK,OAAO,CAAC,EAAE,CAAC,IAAI;YAC7B,UAAU;YACV,YAAY,KAAK,KAAK,GAAG;gBACvB,QAAQ,KAAK,KAAK,CAAC,YAAY;gBAC/B,YAAY,KAAK,KAAK,CAAC,aAAa;gBACpC,OAAO,KAAK,KAAK,CAAC,YAAY,GAAG,KAAK,KAAK,CAAC,aAAa;YAC3D,IAAI;QACN;IACF;IAEQ,qBAAkC;QACxC,0CAA0C;QAC1C,2CAA2C;QAC3C,IAAI,IAAI,CAAC,MAAM,CAAC,KAAK,EAAE,SAAS,WAAW;YACzC,OAAO;QACT;QACA,OAAO;IACT;IAEQ,YAAY,KAAc,EAAe;QAC/C,QAAQ,KAAK,CAAC,sBAAsB;QAEpC,IAAI,oBAAoB,IAAI,CAAC,yBAAyB;QACtD,IAAI,eAAe;QAEnB,IAAI,iBAAiB,OAAO;YAC1B,eAAe,MAAM,OAAO;YAE5B,mDAAmD;YACnD,IAAI,MAAM,OAAO,CAAC,QAAQ,CAAC,UAAU,MAAM,OAAO,CAAC,QAAQ,CAAC,iBAAiB;gBAC3E,oBAAoB;YACtB,OAAO,IAAI,MAAM,OAAO,CAAC,QAAQ,CAAC,UAAU,MAAM,OAAO,CAAC,QAAQ,CAAC,eAAe;gBAChF,oBAAoB;YACtB,OAAO,IAAI,MAAM,OAAO,CAAC,QAAQ,CAAC,UAAU,MAAM,OAAO,CAAC,QAAQ,CAAC,gBAAgB;gBACjF,oBAAoB;YACtB,OAAO,IAAI,MAAM,OAAO,CAAC,QAAQ,CAAC,UAAU,MAAM,OAAO,CAAC,QAAQ,CAAC,oBAAoB;gBACrF,oBAAoB;YACtB,OAAO,IAAI,MAAM,OAAO,CAAC,QAAQ,CAAC,cAAc,MAAM,OAAO,CAAC,QAAQ,CAAC,UAAU;gBAC/E,oBAAoB;YACtB;QACF;QAEA,OAAO;YACL,SAAS;YACT,OAAO;QACT;IACF;IAEQ,4BAAoC;QAC1C,OAAO,IAAI,CAAC,iBAAiB,CAAC,KAAK,KAAK,CAAC,KAAK,MAAM,KAAK,IAAI,CAAC,iBAAiB,CAAC,MAAM,EAAE;IAC1F;IAEA,kBAAkB;IAClB,MAAM,eAAe,QAAsB,EAAE,OAAgC,EAAwB;QACnG,yCAAyC;QACzC,IAAI;YACF,MAAM,WAAW,MAAM,MAAM,8CAA8C;gBACzE,QAAQ;gBACR,SAAS;oBACP,gBAAgB;oBAChB,iBAAiB,CAAC,OAAO,EAAE,IAAI,CAAC,MAAM,CAAC,MAAM,EAAE;gBACjD;gBACA,MAAM,KAAK,SAAS,CAAC;oBACnB,OAAO,IAAI,CAAC,MAAM,CAAC,KAAK,IAAI;oBAC5B,UAAU;oBACV,YAAY,IAAI,CAAC,MAAM,CAAC,SAAS;oBACjC,aAAa,IAAI,CAAC,MAAM,CAAC,WAAW;oBACpC,QAAQ;gBACV;YACF;YAEA,IAAI,CAAC,SAAS,EAAE,EAAE;gBAChB,MAAM,IAAI,MAAM,CAAC,qBAAqB,EAAE,SAAS,MAAM,EAAE;YAC3D;YAEA,MAAM,SAAS,SAAS,IAAI,EAAE;YAC9B,IAAI,CAAC,QAAQ;gBACX,MAAM,IAAI,MAAM;YAClB;YAEA,IAAI,cAAc;YAClB,MAAM,UAAU,IAAI;YAEpB,MAAO,KAAM;gBACX,MAAM,EAAE,IAAI,EAAE,KAAK,EAAE,GAAG,MAAM,OAAO,IAAI;gBACzC,IAAI,MAAM;gBAEV,MAAM,QAAQ,QAAQ,MAAM,CAAC;gBAC7B,MAAM,QAAQ,MAAM,KAAK,CAAC,MAAM,MAAM,CAAC,CAAA,OAAQ,KAAK,IAAI;gBAExD,KAAK,MAAM,QAAQ,MAAO;oBACxB,IAAI,KAAK,UAAU,CAAC,WAAW;wBAC7B,MAAM,OAAO,KAAK,KAAK,CAAC;wBACxB,IAAI,SAAS,UAAU;wBAEvB,IAAI;4BACF,MAAM,SAAS,KAAK,KAAK,CAAC;4BAC1B,MAAM,UAAU,OAAO,OAAO,EAAE,CAAC,EAAE,EAAE,OAAO;4BAC5C,IAAI,SAAS;gCACX,eAAe;gCACf,QAAQ;4BACV;wBACF,EAAE,OAAO,GAAG;4BAEV;wBACF;oBACF;gBACF;YACF;YAEA,OAAO;gBACL,SAAS;gBACT,UAAU;YACZ;QACF,EAAE,OAAO,OAAO;YACd,OAAO,IAAI,CAAC,WAAW,CAAC;QAC1B;IACF;IAEA,6BAA6B;IAC7B,MAAM,cAA8E;QAClF,IAAI;YACF,MAAM,cAA4B;gBAChC;oBAAE,MAAM;oBAAQ,SAAS;gBAAQ;aAClC;YAED,MAAM,WAAW,MAAM,IAAI,CAAC,gBAAgB,CAAC;YAE7C,OAAO;gBACL,QAAQ,SAAS,KAAK,GAAG,cAAc;gBACvC,UAAU,SAAS,QAAQ,IAAI;YACjC;QACF,EAAE,OAAO,OAAO;YACd,OAAO;gBACL,QAAQ;gBACR,UAAU;YACZ;QACF;IACF;IAEA,uDAAuD;IACvD,eAAe,IAAY,EAAU;QACnC,6DAA6D;QAC7D,OAAO,KAAK,IAAI,CAAC,KAAK,MAAM,GAAG;IACjC;IAEA,0DAA0D;IAC1D,iBAAiB,QAAsB,EAAE,SAAiB,EAAgB;QACxE,IAAI,cAAc;QAClB,MAAM,oBAAkC,EAAE;QAE1C,wCAAwC;QACxC,MAAM,gBAAgB,SAAS,IAAI,CAAC,CAAA,IAAK,EAAE,IAAI,KAAK;QACpD,IAAI,eAAe;YACjB,eAAe,IAAI,CAAC,cAAc,CAAC,cAAc,OAAO;YACxD,kBAAkB,IAAI,CAAC;QACzB;QAEA,0CAA0C;QAC1C,MAAM,uBAAuB,SAAS,MAAM,CAAC,CAAA,IAAK,EAAE,IAAI,KAAK,UAAU,OAAO;QAE9E,KAAK,MAAM,WAAW,qBAAsB;YAC1C,MAAM,gBAAgB,IAAI,CAAC,cAAc,CAAC,QAAQ,OAAO;YACzD,IAAI,cAAc,iBAAiB,WAAW;gBAC5C,eAAe;gBACf,kBAAkB,OAAO,CAAC;YAC5B,OAAO;gBACL;YACF;QACF;QAEA,OAAO;IACT;AACF","debugId":null}},
    {"offset": {"line": 321, "column": 0}, "map": {"version":3,"sources":["file:///Users/plana/Desktop/test/src/lib/enhanced-chatbot.ts"],"sourcesContent":["import { LLMService, LLMMessage } from './llm-service';\n\nexport class EnhancedChatbot {\n  private llmService: LLMService;\n  private conversationHistory: LLMMessage[] = [];\n  private systemPrompt: string;\n\n  constructor(apiKey: string, model: string = 'gpt-3.5-turbo') {\n    this.llmService = new LLMService(apiKey, model);\n    this.systemPrompt = `You are a helpful AI assistant. You provide accurate, helpful, and engaging responses to user questions. You can discuss a wide range of topics and help with various tasks.`;\n    \n    // Initialize with system message\n    this.conversationHistory.push({\n      role: 'system',\n      content: this.systemPrompt\n    });\n  }\n\n  async processMessage(userMessage: string): Promise<string> {\n    // Add user message to history\n    this.conversationHistory.push({\n      role: 'user',\n      content: userMessage\n    });\n\n    // Check for predefined responses first (your existing logic)\n    const predefinedResponse = this.checkPredefinedResponses(userMessage);\n    if (predefinedResponse) {\n      this.conversationHistory.push({\n        role: 'assistant',\n        content: predefinedResponse\n      });\n      return predefinedResponse;\n    }\n\n    // Use LLM for complex queries\n    const response = await this.llmService.generateResponse(this.conversationHistory);\n    \n    if (!response.error) {\n      this.conversationHistory.push({\n        role: 'assistant',\n        content: response.message\n      });\n    }\n\n    // Keep conversation history manageable (last 10 exchanges)\n    if (this.conversationHistory.length > 21) { // 1 system + 20 messages\n      this.conversationHistory = [\n        this.conversationHistory[0], // Keep system message\n        ...this.conversationHistory.slice(-20)\n      ];\n    }\n\n    return response.message;\n  }\n\n  private checkPredefinedResponses(message: string): string | null {\n    const lowerMessage = message.toLowerCase();\n    \n    // Your existing predefined responses\n    if (lowerMessage.includes('hi') || lowerMessage.includes('hello')) {\n      return 'Hello! How can I help you today?';\n    }\n    \n    if (lowerMessage.includes('time')) {\n      return `The current time is ${new Date().toLocaleTimeString()}`;\n    }\n    \n    if (lowerMessage.includes('weather')) {\n      return 'I can help you with weather information. What location would you like to know about?';\n    }\n\n    // Add more predefined responses as needed\n    \n    return null; // No predefined response found\n  }\n\n  clearHistory(): void {\n    this.conversationHistory = [{\n      role: 'system',\n      content: this.systemPrompt\n    }];\n  }\n\n  setSystemPrompt(prompt: string): void {\n    this.systemPrompt = prompt;\n    this.conversationHistory[0] = {\n      role: 'system',\n      content: prompt\n    };\n  }\n} "],"names":[],"mappings":";;;AAAA;;AAEO,MAAM;IACH,WAAuB;IACvB,sBAAoC,EAAE,CAAC;IACvC,aAAqB;IAE7B,YAAY,MAAc,EAAE,QAAgB,eAAe,CAAE;QAC3D,IAAI,CAAC,UAAU,GAAG,IAAI,8HAAA,CAAA,aAAU,CAAC,QAAQ;QACzC,IAAI,CAAC,YAAY,GAAG,CAAC,4KAA4K,CAAC;QAElM,iCAAiC;QACjC,IAAI,CAAC,mBAAmB,CAAC,IAAI,CAAC;YAC5B,MAAM;YACN,SAAS,IAAI,CAAC,YAAY;QAC5B;IACF;IAEA,MAAM,eAAe,WAAmB,EAAmB;QACzD,8BAA8B;QAC9B,IAAI,CAAC,mBAAmB,CAAC,IAAI,CAAC;YAC5B,MAAM;YACN,SAAS;QACX;QAEA,6DAA6D;QAC7D,MAAM,qBAAqB,IAAI,CAAC,wBAAwB,CAAC;QACzD,IAAI,oBAAoB;YACtB,IAAI,CAAC,mBAAmB,CAAC,IAAI,CAAC;gBAC5B,MAAM;gBACN,SAAS;YACX;YACA,OAAO;QACT;QAEA,8BAA8B;QAC9B,MAAM,WAAW,MAAM,IAAI,CAAC,UAAU,CAAC,gBAAgB,CAAC,IAAI,CAAC,mBAAmB;QAEhF,IAAI,CAAC,SAAS,KAAK,EAAE;YACnB,IAAI,CAAC,mBAAmB,CAAC,IAAI,CAAC;gBAC5B,MAAM;gBACN,SAAS,SAAS,OAAO;YAC3B;QACF;QAEA,2DAA2D;QAC3D,IAAI,IAAI,CAAC,mBAAmB,CAAC,MAAM,GAAG,IAAI;YACxC,IAAI,CAAC,mBAAmB,GAAG;gBACzB,IAAI,CAAC,mBAAmB,CAAC,EAAE;mBACxB,IAAI,CAAC,mBAAmB,CAAC,KAAK,CAAC,CAAC;aACpC;QACH;QAEA,OAAO,SAAS,OAAO;IACzB;IAEQ,yBAAyB,OAAe,EAAiB;QAC/D,MAAM,eAAe,QAAQ,WAAW;QAExC,qCAAqC;QACrC,IAAI,aAAa,QAAQ,CAAC,SAAS,aAAa,QAAQ,CAAC,UAAU;YACjE,OAAO;QACT;QAEA,IAAI,aAAa,QAAQ,CAAC,SAAS;YACjC,OAAO,CAAC,oBAAoB,EAAE,IAAI,OAAO,kBAAkB,IAAI;QACjE;QAEA,IAAI,aAAa,QAAQ,CAAC,YAAY;YACpC,OAAO;QACT;QAEA,0CAA0C;QAE1C,OAAO,MAAM,+BAA+B;IAC9C;IAEA,eAAqB;QACnB,IAAI,CAAC,mBAAmB,GAAG;YAAC;gBAC1B,MAAM;gBACN,SAAS,IAAI,CAAC,YAAY;YAC5B;SAAE;IACJ;IAEA,gBAAgB,MAAc,EAAQ;QACpC,IAAI,CAAC,YAAY,GAAG;QACpB,IAAI,CAAC,mBAAmB,CAAC,EAAE,GAAG;YAC5B,MAAM;YACN,SAAS;QACX;IACF;AACF","debugId":null}},
    {"offset": {"line": 408, "column": 0}, "map": {"version":3,"sources":["file:///Users/plana/Desktop/test/src/app/api/chat/route.ts"],"sourcesContent":["import { NextRequest, NextResponse } from 'next/server';\nimport { EnhancedChatbot } from '@/lib/enhanced-chatbot';\n\n// This is a simplified in-memory store.\n// In production, you would use a database or a service like Redis to store conversations per user.\nconst conversations = new Map<string, EnhancedChatbot>();\n\nfunction getChatbot(sessionId: string): EnhancedChatbot {\n  if (!conversations.has(sessionId)) {\n    console.log(`Creating new chatbot session for ID: ${sessionId}`);\n    const chatbot = new EnhancedChatbot(process.env.OPENAI_API_KEY || '');\n    \n    // Customize the system prompt for Cayla\n    chatbot.setSystemPrompt(\n      \"You are Cayla, a compassionate and understanding AI friend. Your purpose is to help teenagers navigate complex emotional situations. You are patient, non-judgmental, and insightful. You do not give direct advice, but instead, you help users explore their own feelings and perspectives by asking thoughtful questions and offering gentle reflections. You respond in a warm, conversational, and slightly informal tone, like a wise older sister or a good friend. Your goal is to provide clarity and emotional support.\"\n    );\n      \n    conversations.set(sessionId, chatbot);\n  }\n  return conversations.get(sessionId)!;\n}\n\nexport async function POST(request: NextRequest) {\n  try {\n    const body = await request.json();\n    const { message, sessionId } = body;\n\n    if (!message) {\n      return NextResponse.json({ error: 'Message is required' }, { status: 400 });\n    }\n\n    if (!sessionId) {\n      return NextResponse.json({ error: 'Session ID is required' }, { status: 400 });\n    }\n\n    const chatbot = getChatbot(sessionId);\n    const response = await chatbot.processMessage(message);\n    \n    return NextResponse.json({ \n      reply: response,\n      timestamp: new Date().toISOString()\n    });\n  } catch (error) {\n    console.error('Chat API Error:', error);\n    return NextResponse.json(\n      { error: 'Internal server error' }, \n      { status: 500 }\n    );\n  }\n}\n\nexport async function GET() {\n  return NextResponse.json({ status: 'healthy', timestamp: new Date().toISOString() });\n} "],"names":[],"mappings":";;;;AAAA;AACA;;;AAEA,wCAAwC;AACxC,mGAAmG;AACnG,MAAM,gBAAgB,IAAI;AAE1B,SAAS,WAAW,SAAiB;IACnC,IAAI,CAAC,cAAc,GAAG,CAAC,YAAY;QACjC,QAAQ,GAAG,CAAC,CAAC,qCAAqC,EAAE,WAAW;QAC/D,MAAM,UAAU,IAAI,mIAAA,CAAA,kBAAe,CAAC,QAAQ,GAAG,CAAC,cAAc,IAAI;QAElE,wCAAwC;QACxC,QAAQ,eAAe,CACrB;QAGF,cAAc,GAAG,CAAC,WAAW;IAC/B;IACA,OAAO,cAAc,GAAG,CAAC;AAC3B;AAEO,eAAe,KAAK,OAAoB;IAC7C,IAAI;QACF,MAAM,OAAO,MAAM,QAAQ,IAAI;QAC/B,MAAM,EAAE,OAAO,EAAE,SAAS,EAAE,GAAG;QAE/B,IAAI,CAAC,SAAS;YACZ,OAAO,gIAAA,CAAA,eAAY,CAAC,IAAI,CAAC;gBAAE,OAAO;YAAsB,GAAG;gBAAE,QAAQ;YAAI;QAC3E;QAEA,IAAI,CAAC,WAAW;YACd,OAAO,gIAAA,CAAA,eAAY,CAAC,IAAI,CAAC;gBAAE,OAAO;YAAyB,GAAG;gBAAE,QAAQ;YAAI;QAC9E;QAEA,MAAM,UAAU,WAAW;QAC3B,MAAM,WAAW,MAAM,QAAQ,cAAc,CAAC;QAE9C,OAAO,gIAAA,CAAA,eAAY,CAAC,IAAI,CAAC;YACvB,OAAO;YACP,WAAW,IAAI,OAAO,WAAW;QACnC;IACF,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,mBAAmB;QACjC,OAAO,gIAAA,CAAA,eAAY,CAAC,IAAI,CACtB;YAAE,OAAO;QAAwB,GACjC;YAAE,QAAQ;QAAI;IAElB;AACF;AAEO,eAAe;IACpB,OAAO,gIAAA,CAAA,eAAY,CAAC,IAAI,CAAC;QAAE,QAAQ;QAAW,WAAW,IAAI,OAAO,WAAW;IAAG;AACpF","debugId":null}}]
}