{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 6, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"","debugId":null}},
    {"offset": {"line": 60, "column": 0}, "map": {"version":3,"sources":["file:///Users/plana/Desktop/test/src/lib/ai-provider.ts"],"sourcesContent":["// lib/ai-providers.ts\n\n// Custom Error Types\nexport class AIProviderError extends Error {\n  constructor(message: string, public status: number, public provider: string) {\n    super(message);\n    this.name = 'AIProviderError';\n  }\n}\n\nexport class AuthError extends AIProviderError {\n  constructor(provider: string) {\n    super('Authentication error. Please check your API key.', 401, provider);\n    this.name = 'AuthError';\n  }\n}\n\nexport class RateLimitError extends AIProviderError {\n  constructor(provider: string) {\n    super(\"I'm receiving too many requests right now. Please try again in a moment.\", 429, provider);\n    this.name = 'RateLimitError';\n  }\n}\n\nexport interface ChatMessage {\n  role: 'user' | 'assistant' | 'system';\n  content: string;\n}\n\nexport interface AIResponse {\n  message: string;\n  error?: string;\n  provider?: string;\n}\n\n// OpenAI Integration\nexport class OpenAIService {\n  private apiKey: string;\n  private baseUrl: string = 'https://api.openai.com/v1';\n\n  constructor(apiKey: string) {\n    this.apiKey = apiKey;\n  }\n\n  async generateResponse(\n    messages: ChatMessage[],\n    model: string = 'gpt-3.5-turbo'\n  ): Promise<AIResponse> {\n    try {\n      const response = await fetch(`${this.baseUrl}/chat/completions`, {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json',\n          'Authorization': `Bearer ${this.apiKey}`,\n        },\n        body: JSON.stringify({\n          model: model,\n          messages: messages,\n          max_tokens: 1000,\n          temperature: 0.7,\n          stream: false,\n        }),\n      });\n\n      if (!response.ok) {\n        const errorData = await response.json().catch(() => ({}));\n        const errorMessage = errorData.error?.message || `Unknown error with status ${response.status}`;\n        console.error(`OpenAI API Error: ${response.status}`, errorData);\n\n        if (response.status === 401) {\n          throw new AuthError('OpenAI');\n        }\n        if (response.status === 429) {\n          throw new RateLimitError('OpenAI');\n        }\n\n        throw new AIProviderError(errorMessage, response.status, 'OpenAI');\n      }\n\n      const data = await response.json();\n      return {\n        message: data.choices[0].message.content,\n        provider: 'OpenAI',\n      };\n    } catch (error) {\n      console.error('Failed to generate response from OpenAI:', error);\n      if (error instanceof AIProviderError) {\n        throw error;\n      }\n      throw new AIProviderError(\n        'An unexpected error occurred while connecting to the AI service.',\n        503,\n        'OpenAI'\n      );\n    }\n  }\n\n  // For streaming responses (optional)\n  async generateStreamResponse(\n    messages: ChatMessage[],\n    onChunk: (chunk: string) => void,\n    model: string = 'gpt-3.5-turbo'\n  ): Promise<AIResponse> {\n    try {\n      const response = await fetch(`${this.baseUrl}/chat/completions`, {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json',\n          'Authorization': `Bearer ${this.apiKey}`,\n        },\n        body: JSON.stringify({\n          model: model,\n          messages: messages,\n          max_tokens: 1000,\n          temperature: 0.7,\n          stream: true,\n        }),\n      });\n\n      if (!response.ok) {\n        throw new Error(`OpenAI API Error: ${response.status}`);\n      }\n\n      const reader = response.body?.getReader();\n      if (!reader) throw new Error('No response body');\n\n      let fullMessage = '';\n      \n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n\n        const chunk = new TextDecoder().decode(value);\n        const lines = chunk.split('\\n').filter(line => line.trim());\n\n        for (const line of lines) {\n          if (line.startsWith('data: ')) {\n            const data = line.slice(6);\n            if (data === '[DONE]') continue;\n\n            try {\n              const parsed = JSON.parse(data);\n              const content = parsed.choices[0]?.delta?.content;\n              if (content) {\n                fullMessage += content;\n                onChunk(content);\n              }\n            } catch (e) {\n              // Skip invalid JSON\n            }\n          }\n        }\n      }\n\n      return {\n        message: fullMessage,\n        provider: 'OpenAI',\n      };\n    } catch (error) {\n      console.error('OpenAI Streaming Error:', error);\n      return {\n        message: 'Sorry, I encountered an error with OpenAI streaming.',\n        error: error instanceof Error ? error.message : 'Unknown error',\n        provider: 'OpenAI',\n      };\n    }\n  }\n}\n\n// Hugging Face Integration\nexport class HuggingFaceService {\n  private apiKey: string;\n  private baseUrl: string = 'https://api-inference.huggingface.co/models';\n\n  constructor(apiKey: string) {\n    this.apiKey = apiKey;\n  }\n\n  async generateResponse(\n    messages: ChatMessage[],\n    model: string = 'microsoft/DialoGPT-large'\n  ): Promise<AIResponse> {\n    try {\n      // Convert messages to a single input for most HF models\n      const input = this.formatMessagesForHF(messages);\n\n      const response = await fetch(`${this.baseUrl}/${model}`, {\n        method: 'POST',\n        headers: {\n          'Authorization': `Bearer ${this.apiKey}`,\n          'Content-Type': 'application/json',\n        },\n        body: JSON.stringify({\n          inputs: input,\n          parameters: {\n            max_length: 1000,\n            temperature: 0.7,\n            do_sample: true,\n            top_p: 0.9,\n          },\n        }),\n      });\n\n      if (!response.ok) {\n        const errorData = await response.json().catch(() => ({}));\n        throw new Error(`Hugging Face API Error: ${response.status} - ${errorData.error || 'Unknown error'}`);\n      }\n\n      const data = await response.json();\n      \n      // Handle different response formats\n      let message = '';\n      if (Array.isArray(data) && data[0]?.generated_text) {\n        message = data[0].generated_text.replace(input, '').trim();\n      } else if (data.generated_text) {\n        message = data.generated_text.replace(input, '').trim();\n      } else {\n        message = 'I received an unexpected response format.';\n      }\n\n      return {\n        message: message || 'I apologize, but I couldn\\'t generate a proper response.',\n        provider: 'Hugging Face',\n      };\n    } catch (error) {\n      console.error('Hugging Face API Error:', error);\n      return {\n        message: 'Sorry, I encountered an error with Hugging Face. Please try again.',\n        error: error instanceof Error ? error.message : 'Unknown error',\n        provider: 'Hugging Face',\n      };\n    }\n  }\n\n  // For chat-specific models like Llama, Mistral, etc.\n  async generateChatResponse(\n    messages: ChatMessage[],\n    model: string = 'meta-llama/Llama-2-7b-chat-hf'\n  ): Promise<AIResponse> {\n    try {\n      const response = await fetch(`${this.baseUrl}/${model}`, {\n        method: 'POST',\n        headers: {\n          'Authorization': `Bearer ${this.apiKey}`,\n          'Content-Type': 'application/json',\n        },\n        body: JSON.stringify({\n          inputs: {\n            messages: messages,\n          },\n          parameters: {\n            max_new_tokens: 500,\n            temperature: 0.7,\n            do_sample: true,\n          },\n        }),\n      });\n\n      if (!response.ok) {\n        throw new Error(`Hugging Face Chat API Error: ${response.status}`);\n      }\n\n      const data = await response.json();\n      \n      return {\n        message: data.choices?.[0]?.message?.content || data.generated_text || 'No response generated.',\n        provider: 'Hugging Face',\n      };\n    } catch (error) {\n      console.error('Hugging Face Chat API Error:', error);\n      return {\n        message: 'Sorry, I encountered an error with Hugging Face chat model.',\n        error: error instanceof Error ? error.message : 'Unknown error',\n        provider: 'Hugging Face',\n      };\n    }\n  }\n\n  private formatMessagesForHF(messages: ChatMessage[]): string {\n    // Format conversation for traditional text generation models\n    return messages\n      .filter(msg => msg.role !== 'system')\n      .map(msg => `${msg.role === 'user' ? 'Human' : 'Assistant'}: ${msg.content}`)\n      .join('\\n') + '\\nAssistant:';\n  }\n}\n\n// Enhanced Chatbot with multiple providers\nexport class EnhancedChatbot {\n  private openaiService?: OpenAIService;\n  private huggingfaceService?: HuggingFaceService;\n  private conversationHistory: ChatMessage[] = [];\n  private currentProvider: 'openai' | 'huggingface';\n  private systemPrompt: string;\n\n  constructor(\n    config: {\n      openaiKey?: string;\n      huggingfaceKey?: string;\n      provider?: 'openai' | 'huggingface';\n      systemPrompt?: string;\n    }\n  ) {\n    if (config.openaiKey) {\n      this.openaiService = new OpenAIService(config.openaiKey);\n    }\n    \n    if (config.huggingfaceKey) {\n      this.huggingfaceService = new HuggingFaceService(config.huggingfaceKey);\n    }\n\n    this.currentProvider = config.provider || 'openai';\n    this.systemPrompt = config.systemPrompt || \n      'You are a helpful AI assistant. Provide accurate, helpful, and engaging responses.';\n\n    // Initialize conversation with system message\n    this.conversationHistory.push({\n      role: 'system',\n      content: this.systemPrompt\n    });\n  }\n\n  async processMessage(\n    userMessage: string, \n    options: { short?: boolean } = {}\n  ): Promise<AIResponse> {\n    // Add user message to history\n    this.conversationHistory.push({\n      role: 'user',\n      content: options.short \n        ? `Please answer the following concisely, in one or two sentences at most. ${userMessage}`\n        : userMessage\n    });\n\n    // Check for predefined responses first\n    const predefinedResponse = this.checkPredefinedResponses(userMessage);\n    if (predefinedResponse) {\n      this.conversationHistory.push({\n        role: 'assistant',\n        content: predefinedResponse\n      });\n      return {\n        message: predefinedResponse,\n        provider: 'Predefined'\n      };\n    }\n\n    // Use AI service\n    let response: AIResponse;\n    \n    if (this.currentProvider === 'openai' && this.openaiService) {\n        response = await this.openaiService.generateResponse(this.conversationHistory);\n    } else if (this.currentProvider === 'huggingface' && this.huggingfaceService) {\n        response = await this.huggingfaceService.generateResponse(this.conversationHistory);\n    } else {\n        response = {\n        message: 'No AI provider configured. Please check your API keys.',\n        error: 'No provider available'\n        };\n    }\n\n    // Add assistant response to history if successful\n    if (!response.error) {\n      this.conversationHistory.push({\n        role: 'assistant',\n        content: response.message\n      });\n    }\n\n    // Keep conversation history manageable\n    this.trimConversationHistory();\n\n    return response;\n  }\n\n  switchProvider(provider: 'openai' | 'huggingface'): boolean {\n    if (provider === 'openai' && this.openaiService) {\n      this.currentProvider = 'openai';\n      return true;\n    } else if (provider === 'huggingface' && this.huggingfaceService) {\n      this.currentProvider = 'huggingface';\n      return true;\n    }\n    return false;\n  }\n\n  getCurrentProvider(): string {\n    return this.currentProvider;\n  }\n\n  private checkPredefinedResponses(message: string): string | null {\n    const lowerMessage = message.toLowerCase();\n    \n    // Quick responses for common queries\n    if (lowerMessage.match(/^(hi|hello|hey)$/)) {\n      return 'Hello! How can I help you today?';\n    }\n    \n    if (lowerMessage.includes('time')) {\n      return `The current time is ${new Date().toLocaleTimeString()}`;\n    }\n    \n    if (lowerMessage.includes('date')) {\n      return `Today's date is ${new Date().toLocaleDateString()}`;\n    }\n\n    if (lowerMessage.includes('switch to openai')) {\n      const switched = this.switchProvider('openai');\n      return switched ? 'Switched to OpenAI provider.' : 'OpenAI provider not available.';\n    }\n\n    if (lowerMessage.includes('switch to huggingface')) {\n      const switched = this.switchProvider('huggingface');\n      return switched ? 'Switched to Hugging Face provider.' : 'Hugging Face provider not available.';\n    }\n\n    return null;\n  }\n\n  private trimConversationHistory(): void {\n    // Keep last 20 messages + system message\n    if (this.conversationHistory.length > 21) {\n      this.conversationHistory = [\n        this.conversationHistory[0], // Keep system message\n        ...this.conversationHistory.slice(-20)\n      ];\n    }\n  }\n\n  clearHistory(): void {\n    this.conversationHistory = [{\n      role: 'system',\n      content: this.systemPrompt\n    }];\n  }\n\n  getConversationHistory(): ChatMessage[] {\n    return [...this.conversationHistory];\n  }\n}"],"names":[],"mappings":"AAAA,sBAAsB;AAEtB,qBAAqB;;;;;;;;;AACd,MAAM,wBAAwB;;;IACnC,YAAY,OAAe,EAAE,AAAO,MAAc,EAAE,AAAO,QAAgB,CAAE;QAC3E,KAAK,CAAC,eAD4B,SAAA,aAAuB,WAAA;QAEzD,IAAI,CAAC,IAAI,GAAG;IACd;AACF;AAEO,MAAM,kBAAkB;IAC7B,YAAY,QAAgB,CAAE;QAC5B,KAAK,CAAC,oDAAoD,KAAK;QAC/D,IAAI,CAAC,IAAI,GAAG;IACd;AACF;AAEO,MAAM,uBAAuB;IAClC,YAAY,QAAgB,CAAE;QAC5B,KAAK,CAAC,4EAA4E,KAAK;QACvF,IAAI,CAAC,IAAI,GAAG;IACd;AACF;AAcO,MAAM;IACH,OAAe;IACf,UAAkB,4BAA4B;IAEtD,YAAY,MAAc,CAAE;QAC1B,IAAI,CAAC,MAAM,GAAG;IAChB;IAEA,MAAM,iBACJ,QAAuB,EACvB,QAAgB,eAAe,EACV;QACrB,IAAI;YACF,MAAM,WAAW,MAAM,MAAM,GAAG,IAAI,CAAC,OAAO,CAAC,iBAAiB,CAAC,EAAE;gBAC/D,QAAQ;gBACR,SAAS;oBACP,gBAAgB;oBAChB,iBAAiB,CAAC,OAAO,EAAE,IAAI,CAAC,MAAM,EAAE;gBAC1C;gBACA,MAAM,KAAK,SAAS,CAAC;oBACnB,OAAO;oBACP,UAAU;oBACV,YAAY;oBACZ,aAAa;oBACb,QAAQ;gBACV;YACF;YAEA,IAAI,CAAC,SAAS,EAAE,EAAE;gBAChB,MAAM,YAAY,MAAM,SAAS,IAAI,GAAG,KAAK,CAAC,IAAM,CAAC,CAAC,CAAC;gBACvD,MAAM,eAAe,UAAU,KAAK,EAAE,WAAW,CAAC,0BAA0B,EAAE,SAAS,MAAM,EAAE;gBAC/F,QAAQ,KAAK,CAAC,CAAC,kBAAkB,EAAE,SAAS,MAAM,EAAE,EAAE;gBAEtD,IAAI,SAAS,MAAM,KAAK,KAAK;oBAC3B,MAAM,IAAI,UAAU;gBACtB;gBACA,IAAI,SAAS,MAAM,KAAK,KAAK;oBAC3B,MAAM,IAAI,eAAe;gBAC3B;gBAEA,MAAM,IAAI,gBAAgB,cAAc,SAAS,MAAM,EAAE;YAC3D;YAEA,MAAM,OAAO,MAAM,SAAS,IAAI;YAChC,OAAO;gBACL,SAAS,KAAK,OAAO,CAAC,EAAE,CAAC,OAAO,CAAC,OAAO;gBACxC,UAAU;YACZ;QACF,EAAE,OAAO,OAAO;YACd,QAAQ,KAAK,CAAC,4CAA4C;YAC1D,IAAI,iBAAiB,iBAAiB;gBACpC,MAAM;YACR;YACA,MAAM,IAAI,gBACR,oEACA,KACA;QAEJ;IACF;IAEA,qCAAqC;IACrC,MAAM,uBACJ,QAAuB,EACvB,OAAgC,EAChC,QAAgB,eAAe,EACV;QACrB,IAAI;YACF,MAAM,WAAW,MAAM,MAAM,GAAG,IAAI,CAAC,OAAO,CAAC,iBAAiB,CAAC,EAAE;gBAC/D,QAAQ;gBACR,SAAS;oBACP,gBAAgB;oBAChB,iBAAiB,CAAC,OAAO,EAAE,IAAI,CAAC,MAAM,EAAE;gBAC1C;gBACA,MAAM,KAAK,SAAS,CAAC;oBACnB,OAAO;oBACP,UAAU;oBACV,YAAY;oBACZ,aAAa;oBACb,QAAQ;gBACV;YACF;YAEA,IAAI,CAAC,SAAS,EAAE,EAAE;gBAChB,MAAM,IAAI,MAAM,CAAC,kBAAkB,EAAE,SAAS,MAAM,EAAE;YACxD;YAEA,MAAM,SAAS,SAAS,IAAI,EAAE;YAC9B,IAAI,CAAC,QAAQ,MAAM,IAAI,MAAM;YAE7B,IAAI,cAAc;YAElB,MAAO,KAAM;gBACX,MAAM,EAAE,IAAI,EAAE,KAAK,EAAE,GAAG,MAAM,OAAO,IAAI;gBACzC,IAAI,MAAM;gBAEV,MAAM,QAAQ,IAAI,cAAc,MAAM,CAAC;gBACvC,MAAM,QAAQ,MAAM,KAAK,CAAC,MAAM,MAAM,CAAC,CAAA,OAAQ,KAAK,IAAI;gBAExD,KAAK,MAAM,QAAQ,MAAO;oBACxB,IAAI,KAAK,UAAU,CAAC,WAAW;wBAC7B,MAAM,OAAO,KAAK,KAAK,CAAC;wBACxB,IAAI,SAAS,UAAU;wBAEvB,IAAI;4BACF,MAAM,SAAS,KAAK,KAAK,CAAC;4BAC1B,MAAM,UAAU,OAAO,OAAO,CAAC,EAAE,EAAE,OAAO;4BAC1C,IAAI,SAAS;gCACX,eAAe;gCACf,QAAQ;4BACV;wBACF,EAAE,OAAO,GAAG;wBACV,oBAAoB;wBACtB;oBACF;gBACF;YACF;YAEA,OAAO;gBACL,SAAS;gBACT,UAAU;YACZ;QACF,EAAE,OAAO,OAAO;YACd,QAAQ,KAAK,CAAC,2BAA2B;YACzC,OAAO;gBACL,SAAS;gBACT,OAAO,iBAAiB,QAAQ,MAAM,OAAO,GAAG;gBAChD,UAAU;YACZ;QACF;IACF;AACF;AAGO,MAAM;IACH,OAAe;IACf,UAAkB,8CAA8C;IAExE,YAAY,MAAc,CAAE;QAC1B,IAAI,CAAC,MAAM,GAAG;IAChB;IAEA,MAAM,iBACJ,QAAuB,EACvB,QAAgB,0BAA0B,EACrB;QACrB,IAAI;YACF,wDAAwD;YACxD,MAAM,QAAQ,IAAI,CAAC,mBAAmB,CAAC;YAEvC,MAAM,WAAW,MAAM,MAAM,GAAG,IAAI,CAAC,OAAO,CAAC,CAAC,EAAE,OAAO,EAAE;gBACvD,QAAQ;gBACR,SAAS;oBACP,iBAAiB,CAAC,OAAO,EAAE,IAAI,CAAC,MAAM,EAAE;oBACxC,gBAAgB;gBAClB;gBACA,MAAM,KAAK,SAAS,CAAC;oBACnB,QAAQ;oBACR,YAAY;wBACV,YAAY;wBACZ,aAAa;wBACb,WAAW;wBACX,OAAO;oBACT;gBACF;YACF;YAEA,IAAI,CAAC,SAAS,EAAE,EAAE;gBAChB,MAAM,YAAY,MAAM,SAAS,IAAI,GAAG,KAAK,CAAC,IAAM,CAAC,CAAC,CAAC;gBACvD,MAAM,IAAI,MAAM,CAAC,wBAAwB,EAAE,SAAS,MAAM,CAAC,GAAG,EAAE,UAAU,KAAK,IAAI,iBAAiB;YACtG;YAEA,MAAM,OAAO,MAAM,SAAS,IAAI;YAEhC,oCAAoC;YACpC,IAAI,UAAU;YACd,IAAI,MAAM,OAAO,CAAC,SAAS,IAAI,CAAC,EAAE,EAAE,gBAAgB;gBAClD,UAAU,IAAI,CAAC,EAAE,CAAC,cAAc,CAAC,OAAO,CAAC,OAAO,IAAI,IAAI;YAC1D,OAAO,IAAI,KAAK,cAAc,EAAE;gBAC9B,UAAU,KAAK,cAAc,CAAC,OAAO,CAAC,OAAO,IAAI,IAAI;YACvD,OAAO;gBACL,UAAU;YACZ;YAEA,OAAO;gBACL,SAAS,WAAW;gBACpB,UAAU;YACZ;QACF,EAAE,OAAO,OAAO;YACd,QAAQ,KAAK,CAAC,2BAA2B;YACzC,OAAO;gBACL,SAAS;gBACT,OAAO,iBAAiB,QAAQ,MAAM,OAAO,GAAG;gBAChD,UAAU;YACZ;QACF;IACF;IAEA,qDAAqD;IACrD,MAAM,qBACJ,QAAuB,EACvB,QAAgB,+BAA+B,EAC1B;QACrB,IAAI;YACF,MAAM,WAAW,MAAM,MAAM,GAAG,IAAI,CAAC,OAAO,CAAC,CAAC,EAAE,OAAO,EAAE;gBACvD,QAAQ;gBACR,SAAS;oBACP,iBAAiB,CAAC,OAAO,EAAE,IAAI,CAAC,MAAM,EAAE;oBACxC,gBAAgB;gBAClB;gBACA,MAAM,KAAK,SAAS,CAAC;oBACnB,QAAQ;wBACN,UAAU;oBACZ;oBACA,YAAY;wBACV,gBAAgB;wBAChB,aAAa;wBACb,WAAW;oBACb;gBACF;YACF;YAEA,IAAI,CAAC,SAAS,EAAE,EAAE;gBAChB,MAAM,IAAI,MAAM,CAAC,6BAA6B,EAAE,SAAS,MAAM,EAAE;YACnE;YAEA,MAAM,OAAO,MAAM,SAAS,IAAI;YAEhC,OAAO;gBACL,SAAS,KAAK,OAAO,EAAE,CAAC,EAAE,EAAE,SAAS,WAAW,KAAK,cAAc,IAAI;gBACvE,UAAU;YACZ;QACF,EAAE,OAAO,OAAO;YACd,QAAQ,KAAK,CAAC,gCAAgC;YAC9C,OAAO;gBACL,SAAS;gBACT,OAAO,iBAAiB,QAAQ,MAAM,OAAO,GAAG;gBAChD,UAAU;YACZ;QACF;IACF;IAEQ,oBAAoB,QAAuB,EAAU;QAC3D,6DAA6D;QAC7D,OAAO,SACJ,MAAM,CAAC,CAAA,MAAO,IAAI,IAAI,KAAK,UAC3B,GAAG,CAAC,CAAA,MAAO,GAAG,IAAI,IAAI,KAAK,SAAS,UAAU,YAAY,EAAE,EAAE,IAAI,OAAO,EAAE,EAC3E,IAAI,CAAC,QAAQ;IAClB;AACF;AAGO,MAAM;IACH,cAA8B;IAC9B,mBAAwC;IACxC,sBAAqC,EAAE,CAAC;IACxC,gBAA0C;IAC1C,aAAqB;IAE7B,YACE,MAKC,CACD;QACA,IAAI,OAAO,SAAS,EAAE;YACpB,IAAI,CAAC,aAAa,GAAG,IAAI,cAAc,OAAO,SAAS;QACzD;QAEA,IAAI,OAAO,cAAc,EAAE;YACzB,IAAI,CAAC,kBAAkB,GAAG,IAAI,mBAAmB,OAAO,cAAc;QACxE;QAEA,IAAI,CAAC,eAAe,GAAG,OAAO,QAAQ,IAAI;QAC1C,IAAI,CAAC,YAAY,GAAG,OAAO,YAAY,IACrC;QAEF,8CAA8C;QAC9C,IAAI,CAAC,mBAAmB,CAAC,IAAI,CAAC;YAC5B,MAAM;YACN,SAAS,IAAI,CAAC,YAAY;QAC5B;IACF;IAEA,MAAM,eACJ,WAAmB,EACnB,UAA+B,CAAC,CAAC,EACZ;QACrB,8BAA8B;QAC9B,IAAI,CAAC,mBAAmB,CAAC,IAAI,CAAC;YAC5B,MAAM;YACN,SAAS,QAAQ,KAAK,GAClB,CAAC,wEAAwE,EAAE,aAAa,GACxF;QACN;QAEA,uCAAuC;QACvC,MAAM,qBAAqB,IAAI,CAAC,wBAAwB,CAAC;QACzD,IAAI,oBAAoB;YACtB,IAAI,CAAC,mBAAmB,CAAC,IAAI,CAAC;gBAC5B,MAAM;gBACN,SAAS;YACX;YACA,OAAO;gBACL,SAAS;gBACT,UAAU;YACZ;QACF;QAEA,iBAAiB;QACjB,IAAI;QAEJ,IAAI,IAAI,CAAC,eAAe,KAAK,YAAY,IAAI,CAAC,aAAa,EAAE;YACzD,WAAW,MAAM,IAAI,CAAC,aAAa,CAAC,gBAAgB,CAAC,IAAI,CAAC,mBAAmB;QACjF,OAAO,IAAI,IAAI,CAAC,eAAe,KAAK,iBAAiB,IAAI,CAAC,kBAAkB,EAAE;YAC1E,WAAW,MAAM,IAAI,CAAC,kBAAkB,CAAC,gBAAgB,CAAC,IAAI,CAAC,mBAAmB;QACtF,OAAO;YACH,WAAW;gBACX,SAAS;gBACT,OAAO;YACP;QACJ;QAEA,kDAAkD;QAClD,IAAI,CAAC,SAAS,KAAK,EAAE;YACnB,IAAI,CAAC,mBAAmB,CAAC,IAAI,CAAC;gBAC5B,MAAM;gBACN,SAAS,SAAS,OAAO;YAC3B;QACF;QAEA,uCAAuC;QACvC,IAAI,CAAC,uBAAuB;QAE5B,OAAO;IACT;IAEA,eAAe,QAAkC,EAAW;QAC1D,IAAI,aAAa,YAAY,IAAI,CAAC,aAAa,EAAE;YAC/C,IAAI,CAAC,eAAe,GAAG;YACvB,OAAO;QACT,OAAO,IAAI,aAAa,iBAAiB,IAAI,CAAC,kBAAkB,EAAE;YAChE,IAAI,CAAC,eAAe,GAAG;YACvB,OAAO;QACT;QACA,OAAO;IACT;IAEA,qBAA6B;QAC3B,OAAO,IAAI,CAAC,eAAe;IAC7B;IAEQ,yBAAyB,OAAe,EAAiB;QAC/D,MAAM,eAAe,QAAQ,WAAW;QAExC,qCAAqC;QACrC,IAAI,aAAa,KAAK,CAAC,qBAAqB;YAC1C,OAAO;QACT;QAEA,IAAI,aAAa,QAAQ,CAAC,SAAS;YACjC,OAAO,CAAC,oBAAoB,EAAE,IAAI,OAAO,kBAAkB,IAAI;QACjE;QAEA,IAAI,aAAa,QAAQ,CAAC,SAAS;YACjC,OAAO,CAAC,gBAAgB,EAAE,IAAI,OAAO,kBAAkB,IAAI;QAC7D;QAEA,IAAI,aAAa,QAAQ,CAAC,qBAAqB;YAC7C,MAAM,WAAW,IAAI,CAAC,cAAc,CAAC;YACrC,OAAO,WAAW,iCAAiC;QACrD;QAEA,IAAI,aAAa,QAAQ,CAAC,0BAA0B;YAClD,MAAM,WAAW,IAAI,CAAC,cAAc,CAAC;YACrC,OAAO,WAAW,uCAAuC;QAC3D;QAEA,OAAO;IACT;IAEQ,0BAAgC;QACtC,yCAAyC;QACzC,IAAI,IAAI,CAAC,mBAAmB,CAAC,MAAM,GAAG,IAAI;YACxC,IAAI,CAAC,mBAAmB,GAAG;gBACzB,IAAI,CAAC,mBAAmB,CAAC,EAAE;mBACxB,IAAI,CAAC,mBAAmB,CAAC,KAAK,CAAC,CAAC;aACpC;QACH;IACF;IAEA,eAAqB;QACnB,IAAI,CAAC,mBAAmB,GAAG;YAAC;gBAC1B,MAAM;gBACN,SAAS,IAAI,CAAC,YAAY;YAC5B;SAAE;IACJ;IAEA,yBAAwC;QACtC,OAAO;eAAI,IAAI,CAAC,mBAAmB;SAAC;IACtC;AACF","debugId":null}},
    {"offset": {"line": 417, "column": 0}, "map": {"version":3,"sources":["file:///Users/plana/Desktop/test/src/lib/sentiment-analysis.ts/proxy.mjs"],"sourcesContent":["import { registerClientReference } from \"react-server-dom-turbopack/server.edge\";\nexport const avatarMap = registerClientReference(\n    function() { throw new Error(\"Attempted to call avatarMap() from the server but avatarMap is on the client. It's not possible to invoke a client function from the server, it can only be rendered as a Component or passed to props of a Client Component.\"); },\n    \"[project]/src/lib/sentiment-analysis.ts <module evaluation>\",\n    \"avatarMap\",\n);\nexport const detectEmotion = registerClientReference(\n    function() { throw new Error(\"Attempted to call detectEmotion() from the server but detectEmotion is on the client. It's not possible to invoke a client function from the server, it can only be rendered as a Component or passed to props of a Client Component.\"); },\n    \"[project]/src/lib/sentiment-analysis.ts <module evaluation>\",\n    \"detectEmotion\",\n);\n"],"names":[],"mappings":";;;;AAAA;;AACO,MAAM,YAAY,CAAA,GAAA,uPAAA,CAAA,0BAAuB,AAAD,EAC3C;IAAa,MAAM,IAAI,MAAM;AAAkO,GAC/P,+DACA;AAEG,MAAM,gBAAgB,CAAA,GAAA,uPAAA,CAAA,0BAAuB,AAAD,EAC/C;IAAa,MAAM,IAAI,MAAM;AAA0O,GACvQ,+DACA","debugId":null}},
    {"offset": {"line": 435, "column": 0}, "map": {"version":3,"sources":["file:///Users/plana/Desktop/test/src/lib/sentiment-analysis.ts/proxy.mjs"],"sourcesContent":["import { registerClientReference } from \"react-server-dom-turbopack/server.edge\";\nexport const avatarMap = registerClientReference(\n    function() { throw new Error(\"Attempted to call avatarMap() from the server but avatarMap is on the client. It's not possible to invoke a client function from the server, it can only be rendered as a Component or passed to props of a Client Component.\"); },\n    \"[project]/src/lib/sentiment-analysis.ts\",\n    \"avatarMap\",\n);\nexport const detectEmotion = registerClientReference(\n    function() { throw new Error(\"Attempted to call detectEmotion() from the server but detectEmotion is on the client. It's not possible to invoke a client function from the server, it can only be rendered as a Component or passed to props of a Client Component.\"); },\n    \"[project]/src/lib/sentiment-analysis.ts\",\n    \"detectEmotion\",\n);\n"],"names":[],"mappings":";;;;AAAA;;AACO,MAAM,YAAY,CAAA,GAAA,uPAAA,CAAA,0BAAuB,AAAD,EAC3C;IAAa,MAAM,IAAI,MAAM;AAAkO,GAC/P,2CACA;AAEG,MAAM,gBAAgB,CAAA,GAAA,uPAAA,CAAA,0BAAuB,AAAD,EAC/C;IAAa,MAAM,IAAI,MAAM;AAA0O,GACvQ,2CACA","debugId":null}},
    {"offset": {"line": 453, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"","debugId":null}},
    {"offset": {"line": 463, "column": 0}, "map": {"version":3,"sources":["file:///Users/plana/Desktop/test/src/app/api/chat/route.ts"],"sourcesContent":["import { NextRequest, NextResponse } from 'next/server';\nimport { EnhancedChatbot, AIProviderError, AuthError, RateLimitError } from '@/lib/ai-provider';\nimport { detectEmotion } from '@/lib/sentiment-analysis';\n\n// This is a simplified in-memory store.\n// In production, you would use a database or a service like Redis to store conversations per user.\nconst conversations = new Map<string, EnhancedChatbot>();\n\nfunction getChatbot(sessionId: string): EnhancedChatbot {\n  if (!conversations.has(sessionId)) {\n    console.log(`Creating new chatbot session for ID: ${sessionId}`);\n    const systemPrompt = \"You are Cayla, a compassionate and understanding AI friend. Your purpose is to help teenagers navigate complex emotional situations. You are patient, non-judgmental, and insightful. You do not give direct advice, but instead, you help users explore their own feelings and perspectives by asking thoughtful questions and offering gentle reflections. You respond in a warm, conversational, and slightly informal tone, like a wise older sister or a good friend. Your goal is to provide clarity and emotional support.\";\n    \n    const chatbot = new EnhancedChatbot({\n      openaiKey: process.env.OPENAI_API_KEY || '',\n      systemPrompt: systemPrompt,\n    });\n      \n    conversations.set(sessionId, chatbot);\n  }\n  return conversations.get(sessionId)!;\n}\n\nexport async function POST(request: NextRequest) {\n  if (!process.env.OPENAI_API_KEY) {\n    console.error('Missing OPENAI_API_KEY environment variable');\n    return new Response(\n      'The AI service is not configured correctly. Please check server logs.',\n      { status: 500 }\n    );\n  }\n\n  try {\n    const body = await request.json();\n    const { messages } = body;\n    \n    // Get metadata from headers\n    const sessionId = request.headers.get('X-Session-Id');\n    const isShort = request.headers.get('X-Short-Mode') === 'true';\n\n    if (!messages || messages.length === 0) {\n      return NextResponse.json({ error: 'Messages are required' }, { status: 400 });\n    }\n    \n    const lastMessage = messages[messages.length - 1];\n    const userContent = lastMessage.content;\n\n    if (!userContent) {\n      return NextResponse.json({ error: 'Message content is required' }, { status: 400 });\n    }\n\n    if (!sessionId) {\n      return NextResponse.json({ error: 'Session ID is required' }, { status: 400 });\n    }\n\n    const chatbot = getChatbot(sessionId);\n    const response = await chatbot.processMessage(userContent, { short: isShort });\n    \n    const emotion = await detectEmotion(response.message);\n    \n    const headers = new Headers();\n    headers.append('X-Emotion', emotion);\n\n    return new Response(response.message, {\n      status: 200,\n      headers: headers,\n    });\n\n  } catch (error) {\n    console.error('Chat API Error:', error);\n\n    if (error instanceof AIProviderError) {\n       return new Response(error.message, { status: error.status });\n    }\n\n    return new Response(\n      'An internal server error occurred.', \n      { status: 500 }\n    );\n  }\n}\n\nexport async function GET() {\n  return NextResponse.json({ status: 'healthy', timestamp: new Date().toISOString() });\n} "],"names":[],"mappings":";;;;AAAA;AACA;AACA;;;;AAEA,wCAAwC;AACxC,mGAAmG;AACnG,MAAM,gBAAgB,IAAI;AAE1B,SAAS,WAAW,SAAiB;IACnC,IAAI,CAAC,cAAc,GAAG,CAAC,YAAY;QACjC,QAAQ,GAAG,CAAC,CAAC,qCAAqC,EAAE,WAAW;QAC/D,MAAM,eAAe;QAErB,MAAM,UAAU,IAAI,8HAAA,CAAA,kBAAe,CAAC;YAClC,WAAW,QAAQ,GAAG,CAAC,cAAc,IAAI;YACzC,cAAc;QAChB;QAEA,cAAc,GAAG,CAAC,WAAW;IAC/B;IACA,OAAO,cAAc,GAAG,CAAC;AAC3B;AAEO,eAAe,KAAK,OAAoB;IAC7C,IAAI,CAAC,QAAQ,GAAG,CAAC,cAAc,EAAE;QAC/B,QAAQ,KAAK,CAAC;QACd,OAAO,IAAI,SACT,yEACA;YAAE,QAAQ;QAAI;IAElB;IAEA,IAAI;QACF,MAAM,OAAO,MAAM,QAAQ,IAAI;QAC/B,MAAM,EAAE,QAAQ,EAAE,GAAG;QAErB,4BAA4B;QAC5B,MAAM,YAAY,QAAQ,OAAO,CAAC,GAAG,CAAC;QACtC,MAAM,UAAU,QAAQ,OAAO,CAAC,GAAG,CAAC,oBAAoB;QAExD,IAAI,CAAC,YAAY,SAAS,MAAM,KAAK,GAAG;YACtC,OAAO,gIAAA,CAAA,eAAY,CAAC,IAAI,CAAC;gBAAE,OAAO;YAAwB,GAAG;gBAAE,QAAQ;YAAI;QAC7E;QAEA,MAAM,cAAc,QAAQ,CAAC,SAAS,MAAM,GAAG,EAAE;QACjD,MAAM,cAAc,YAAY,OAAO;QAEvC,IAAI,CAAC,aAAa;YAChB,OAAO,gIAAA,CAAA,eAAY,CAAC,IAAI,CAAC;gBAAE,OAAO;YAA8B,GAAG;gBAAE,QAAQ;YAAI;QACnF;QAEA,IAAI,CAAC,WAAW;YACd,OAAO,gIAAA,CAAA,eAAY,CAAC,IAAI,CAAC;gBAAE,OAAO;YAAyB,GAAG;gBAAE,QAAQ;YAAI;QAC9E;QAEA,MAAM,UAAU,WAAW;QAC3B,MAAM,WAAW,MAAM,QAAQ,cAAc,CAAC,aAAa;YAAE,OAAO;QAAQ;QAE5E,MAAM,UAAU,MAAM,CAAA,GAAA,qIAAA,CAAA,gBAAa,AAAD,EAAE,SAAS,OAAO;QAEpD,MAAM,UAAU,IAAI;QACpB,QAAQ,MAAM,CAAC,aAAa;QAE5B,OAAO,IAAI,SAAS,SAAS,OAAO,EAAE;YACpC,QAAQ;YACR,SAAS;QACX;IAEF,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,mBAAmB;QAEjC,IAAI,iBAAiB,8HAAA,CAAA,kBAAe,EAAE;YACnC,OAAO,IAAI,SAAS,MAAM,OAAO,EAAE;gBAAE,QAAQ,MAAM,MAAM;YAAC;QAC7D;QAEA,OAAO,IAAI,SACT,sCACA;YAAE,QAAQ;QAAI;IAElB;AACF;AAEO,eAAe;IACpB,OAAO,gIAAA,CAAA,eAAY,CAAC,IAAI,CAAC;QAAE,QAAQ;QAAW,WAAW,IAAI,OAAO,WAAW;IAAG;AACpF","debugId":null}}]
}